{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib3 \n",
    "import re\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk, string, numpy,math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def StemTokens(tokens):\n",
    "     return [stemmer.stem(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def StemNormalize(text):\n",
    "     return StemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "    \n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "     return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "     return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "    \n",
    "def idf(n,df):\n",
    "    result = math.log((n+1.0)/(df+1.0)) + 1\n",
    "    return result\n",
    "\n",
    "\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visible(element):\n",
    "            if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "                return False\n",
    "            elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "                return False\n",
    "            return True\n",
    "def second_largest(numbers):\n",
    "    count = 0\n",
    "    m1 = m2 = float('-inf')\n",
    "    for x in numbers:\n",
    "        count += 1\n",
    "        if x > m2:\n",
    "            if x >= m1:\n",
    "                m1, m2 = x, m1            \n",
    "            else:\n",
    "                m2 = x\n",
    "    return m2 if count >= 2 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(claim,article_snippets):\n",
    "    docs=[]\n",
    "    docs.append(claim)\n",
    "    for i in range(0,len(article_snippets)):\n",
    "        docs.append(article_snippets[i])\n",
    "    LemVectorizer = CountVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    LemVectorizer.fit_transform(docs)\n",
    "    tf_matrix = LemVectorizer.transform(docs).toarray()\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    def cos_similarity(textlist):\n",
    "        tfidf = TfidfVec.fit_transform(textlist)\n",
    "        return (tfidf * tfidf.T).toarray()\n",
    "    cos_sim_mat=cos_similarity(docs)\n",
    "    \n",
    "    a= list(cos_sim_mat[0])\n",
    "    \n",
    "    k=second_largest(a)\n",
    "    l=a.index(k)\n",
    "#     print (\"k_index \\t\"+str(k))\n",
    "#     print(docs[l])\n",
    "    \n",
    "    return \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(df):\n",
    "    final_article=[]\n",
    "    for i,row in df.iterrows() :\n",
    "        article= row[\"article_link\"]\n",
    "        claim=row[\"claim\"]\n",
    "        file=row[\"claim_file\"]\n",
    "        http = urllib3.PoolManager()\n",
    "        url=article\n",
    "        print (file)\n",
    "        try:\n",
    "            response = http.request('GET', url)\n",
    "        except:\n",
    "            print(\"the row \\t \"+str(i)+\"\\t has been removed\")\n",
    "            df.drop(i)\n",
    "            continue\n",
    "            \n",
    "        print(\"crawl_index \\t\"+str(i))\n",
    "        print(article)\n",
    "        soup=bs(response.data)\n",
    "        data = soup.findAll(text=True)\n",
    "        result = filter(visible, data)\n",
    "        text=list(result)\n",
    "       \n",
    "        text=[e for e in text if e not in ('\\n','ðŸ‘¤',' ')]\n",
    "#         print(text)\n",
    "        if len(text) > 10:\n",
    "#                 df.drop(i)\n",
    "#                 continue\n",
    "#             print(text)\n",
    "            final_text=[]\n",
    "            for i in text:\n",
    "                if len(i) > 100 :\n",
    "#                     print(\"*****************\")\n",
    "#                     print(len(i))\n",
    "#                     print(\"----------\")\n",
    "#                     print (i)\n",
    "#                     print(\"-----------------------------------------------\")\n",
    "                    final_text.append(i)\n",
    "                #print(i)\n",
    "                #print(final_text[i])\n",
    "            if len(final_text)>5 :\n",
    "                article=similarity(claim,final_text) #toDO\n",
    "                final_article.append(article)\n",
    "    #         print (article)\n",
    "        \n",
    "\n",
    "        \n",
    "    column_values = pd.Series(final_article)\n",
    "    df.insert(loc=0,column='article' ,value=column_values)\n",
    "    return dataframe\n",
    "\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/media/sdb/sanjay/IR/debunking-fake-news/Snopes/Snopes\"\n",
    "if os.path.exists(path):\n",
    "    json_files=os.listdir(path)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=('claim_id','claim_file','claim','article_link','article_source','credibility'))\n",
    "j=0\n",
    "k=0\n",
    "for file in json_files: \n",
    "    j=j+1\n",
    "    file_path=path+'/'+file\n",
    "    with open(file_path,'r') as f:\n",
    "        file_data=json.load(f)\n",
    "    if len(file_data[\"Google Results\"][0][\"results\"]) > 7 :\n",
    "        claim=file_data[\"Claim\"] \n",
    "        cred=file_data[\"Credibility\"]\n",
    "        for i in range(0,7):\n",
    "            article=file_data[\"Google Results\"][0][\"results\"][i]['link']\n",
    "            article_source=file_data[\"Google Results\"][0][\"results\"][i][\"domain\"]\n",
    "            df.loc[k]=[k,file,claim,article,article_source,cred]\n",
    "            k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('snopes_refined.csv', sep='\\t')\n",
    "df=pd.read_csv('snopes_refined.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=crawl(df[3000:6000])\n",
    "df_final.to_csv('snopes_final.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
